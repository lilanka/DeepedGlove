\documentclass{article}

\usepackage[margin=0.5in]{geometry}
\usepackage{pgfmath}
\usepackage{pgfcalendar}
\usepackage[utf8]{inputenc}

\usepackage{lucida}
% \usepackage{stix}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{tikz-cd}

% figure support
\usepackage{pgfplots}
\usepackage{import}
\usepackage[shortlabels]{enumitem}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{pdfpages}
\usepackage{transparent}
\newcommand{\incfig}[1]{%
    \def\svgwidth{\columnwidth}
    \import{./figures/}{#1.pdf_tex}
}
\pdfsuppresswarningpagegroup=1

\usepackage{xcolor}
\usepackage{parskip}
\usepackage{soul}

\newcommand{\E}{\mathbb E}
\newcommand{\C}{\mathbb C}
\newcommand{\R}{\mathbb R}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Z}{\mathbb Z}

\usepackage{hyperref}
\hypersetup{hidelinks}
\usepackage{fontawesome}
\usepackage{xifthen}% provides \isempty test
\newcommand\pdfref[3]{%
    \href{phd://open-paper?id=#1&page=#2}{%
    \textup{[\textbf{\ifthenelse{\isempty{#3}}{here}{#3}}]}}%
}
\newcommand\urlref[2]{%
    \href{#1}{\raisebox{0.15ex}{\scriptsize \faLink}\:\textup{\textbf{#2}}}%
}
\newcommand\absolutefileref[2]{%
    \href{run:#1}{\raisebox{0.15ex}{\scriptsize \faFile}\:\textup{\textbf{#2}}}%
}

% this will contain the current date in yyyy-mm-dd format
\def\formatteddate{}
\newcommand\fileref[2]{
    \IfFileExists{./\formatteddate/#1}{
        \absolutefileref{./\formatteddate/#1}{#2}
    }{
        \textcolor{gray}{\absolutefileref{./\formatteddate/#1}{#2}}
    }
}
\newcommand{\xournal}{\fileref{note.xoj}{Handwritten notes}}%

\let\d=\pgfcalendarshorthand
\newcommand\formatdate[2]{\pgfcalendar{cal}{#1}{#1}{#2}}

\newcommand\firstdate{\year-\month-\day+-14}
\newcommand\lastdate{\year-\month-\day}

\newcommand\grayrule{{\color{gray} \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}}}

\begin{document}
\begin{center}
    \huge{DeepedGlove}\\[0.4em]
    \Large{Deep Learning Based Energy Optimization of Glove Manufacturing Plants \((Q_2/W_2)\).}\\[0.2em]
\end{center}

%\newpage
%\tableofcontents

% -----------------------------------------------------------------------------------------------------------
\section{Introduction}
In this project we develop data-driven AI system called DeepedGlove to optimize the energy efficiency of 
glove manufacturing plant. 

The real world manufacturing systems are very complex. It can cause great damage to lives or cost if something goes 
out. Due to that reason the model is in offline setting. Where the agent isn't allowed have interactions with real 
system or perfect simulation environment (The SCADA system provides the collected data).

DeepedGlove consists of 
\begin{itemize}
  \item \emph{Glove Manufacturing Plant Simulator}: Facilate RL training.
  \item \emph{Model Based Offline RL Frameword (MORE)}: Learn policy under safety constraints.
\end{itemize}

\section{Operation Mechanism of Manufacturing Plant}
The manufacturing plant is highly complex system consists of lot of sensors. The general glove manufacturing process 
has \(3\) main stages.
\begin{enumerate}
  \item Glove dipping on polymer tank.
  \item Leaching tank treatment.
  \item Heating oven process.
\end{enumerate}

\section{Model}
We model the DeepedGlove energy optimization problem as a Constrained Markov Decision Process. It precented by a 
tuple \(\left(S, A, T, r, c_{1:m}, \gamma\right)\). Here \(S\) and \(A\) denotes state and action respectively. 
\(T(s_{t+1}|s_t,a_t)\) is transision dynamics. \(r(s_t,a_t) > 0\) is the reward function. \(c_{1:m}(s_t, a_t)\)
are \(m\) cost function. \(\gamma \in (0,1)\) is the discount factor.

\begin{itemize}
  \item \textbf{States \((S)\)}: We use states as temperature \((t)\) of each tank and ovens, rpm values of motors
   \((m)\), level values as \((l)\), and flow speed \((f)\).
  \item \textbf{Actions \((A)\)}: Action values are rpm values of motors. All actions are continuous.
  \item \textbf{Reward Function \((r)\)}: We model the reward as following equation
    \[r_d = (w_1 \times Q_d) + (w_2 \times {GN}_d) - (w_3 \times {EC}_d) - (w_4 \times {HF}_d)\]
    Here 
    \begin{itemize}
      \item \(Q_d\): The quality of the gloves produced in the given day.
      \item \({GN}_d\): Number of gloves manufactured in the given day.
      \item \({EC}_d\): Energy consumption of the plant each day.
      \item \({HF}_d\): Average temperature inside the plant in each day.
    \end{itemize}
    The \(w_i \in [0,1]\) values are set according to priorities.
  \item \textbf{Cost Functions \((c_{1:m})\)}: We model safety constraints as costs. Such as the temperature ranges
  of each section, and motor speed of each sections.
\end{itemize}

The model is trained using historical operational dataset \(B = (s, a, s', r, c_{1:m})\) generated by the operators.
The model learns optimal policy \(\pi^{*}(s)\) from \(B\) that maximize the expected discounted reward 
\[R(\pi) = \E_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t, a_t)\right]\]
While keeping the the expected discounted cost 
\[C(\pi) = \E_{\pi}\left[\sum_{t=0}^{\infty} r^t c(s_t,a_t)\right]\]
below a threashold \(l\).

\subsection{MORE}
This model uses Model-based Offline RL with Restrictive Exploration (MORE)[1] for the agent. MORE uses two types of 
Q-functions for reward maximization \((Q_r)\) and for cost evaluation \((Q_c)\). To handle overesteimate problem and 
panelize the uncertainty of \(Q_r\) MORE uses the clipped double Q method [4] using two \(Q_r\) networks. 
Since \(Q_c\) normally underestimate the cost, it doesn't uses this method. 

The policy optimization is done by Lagrangian relaxation procedure.
\[L(\pi,\lambda) = \E_{s \sim D,a \sim \pi}\left[\min_{j=1,2} Q_{r_j}(s,a) - \lambda (Q_c(s,a) - l)\right]\]
\[(\pi^{*},\lambda^{*}) = \arg \min_{\lambda \ge 0} \max_{\pi} L(\pi, \lambda)\]
\[\lambda \leftarrow {\left[\lambda + \eta \left(\E_{a\sim \pi}\left[Q_c(s,a)\right] - l\right)\right]}^{+}\]

Here \(\lambda\) is the Lagrangian multiplier, \(\eta\) is the step size, and \([x]^{+} = \max\{0,x\}\).

\section{Simmulator}
The simmulator is based on recurrent neural network (RNN), with it's internel cel structure is designed according 
to the actual physical process.

\[Glove\ dipping\ process \Rightarrow Leaching\ tank\ treatment \Rightarrow Heating\ oven\ process\]

Given current state \((s_t)\) and action \((a_t)\) the simmulator model the next state \((s_{t+1}^{'})\), reward \((r_{t+1}^{'})\), 
and the cost \((c_{t+1}^{'})\). 

\section{References}
\begin{enumerate}
  \item \href{https://arxiv.org/pdf/2102.11492.pdf}{DeepThermal: Combustion Optimization for
  Thermal Power Generating Units Using Offline Reinforcement Learning}
  \item \href{https://arxiv.org/pdf/2201.11624.pdf}{LiteLSTM Architecture for Deep Recurrent Neural
  Networks}
  \item \href{https://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf}{Constrained Markov Decision Processes}
  \item \href{https://arxiv.org/pdf/1802.09477v3.pdf}{Addressing Function Approximation Error in Actor-Critic Methods}.
\end{enumerate}

\end{document}
